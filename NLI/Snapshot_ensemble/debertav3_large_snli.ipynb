{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:22:50.242737Z",
     "iopub.status.busy": "2024-05-17T19:22:50.242320Z",
     "iopub.status.idle": "2024-05-17T19:22:51.224164Z",
     "shell.execute_reply": "2024-05-17T19:22:51.223090Z",
     "shell.execute_reply.started": "2024-05-17T19:22:50.242696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 20 10:36:52 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    70W / 400W |   5640MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   24C    P0    52W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    56W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  Off  | 00000000:4D:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    59W / 400W |   9452MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM...  Off  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    76W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM...  Off  | 00000000:8D:00.0 Off |                    0 |\n",
      "| N/A   39C    P0   247W / 400W |  39514MiB / 40960MiB |     86%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM...  Off  | 00000000:C7:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    53W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM...  Off  | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   46C    P0   256W / 400W |  39522MiB / 40960MiB |     88%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    559832      C   .../envs/tsai_env/bin/python     1596MiB |\n",
      "|    0   N/A  N/A    570195      C   .../envs/tsai_env/bin/python     1596MiB |\n",
      "|    0   N/A  N/A   1716425      C   .../envs/torchenv/bin/python      850MiB |\n",
      "|    0   N/A  N/A   1769536      C   .../envs/tsai_env/bin/python     1596MiB |\n",
      "|    3   N/A  N/A    559832      C   .../envs/tsai_env/bin/python     3148MiB |\n",
      "|    3   N/A  N/A    570195      C   .../envs/tsai_env/bin/python     3150MiB |\n",
      "|    3   N/A  N/A   1769536      C   .../envs/tsai_env/bin/python     3152MiB |\n",
      "|    5   N/A  N/A   1247639      C   python                          39512MiB |\n",
      "|    7   N/A  N/A   1157318      C   .../envs/torchenv/bin/python     1430MiB |\n",
      "|    7   N/A  N/A   1508043      C   python                          34896MiB |\n",
      "|    7   N/A  N/A   2682638      C   .../envs/torchenv/bin/python     1598MiB |\n",
      "|    7   N/A  N/A   2722600      C   .../envs/torchenv/bin/python     1596MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:22:51.226583Z",
     "iopub.status.busy": "2024-05-17T19:22:51.226277Z",
     "iopub.status.idle": "2024-05-17T19:22:57.082817Z",
     "shell.execute_reply": "2024-05-17T19:22:57.081874Z",
     "shell.execute_reply.started": "2024-05-17T19:22:51.226556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs:\n",
      "0: NVIDIA A100-SXM4-40GB\n",
      "1: NVIDIA A100-SXM4-40GB\n",
      "2: NVIDIA A100-SXM4-40GB\n",
      "3: NVIDIA A100-SXM4-40GB\n",
      "4: NVIDIA A100-SXM4-40GB\n",
      "5: NVIDIA A100-SXM4-40GB\n",
      "6: NVIDIA A100-SXM4-40GB\n",
      "7: NVIDIA A100-SXM4-40GB\n",
      "Select GPU by entering the device ID (default 0): 0\n",
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Function to list available GPUs and select one\n",
    "def select_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Available GPUs:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"{i}: {torch.cuda.get_device_name(i)}\")\n",
    "        device_id = int(input(\"Select GPU by entering the device ID (default 0): \") or 0)\n",
    "        if device_id < torch.cuda.device_count():\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(device_id)}\")\n",
    "            return torch.device(f\"cuda:{device_id}\")\n",
    "        else:\n",
    "            print(f\"Invalid device ID. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            return torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        print(\"No GPU available. Using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Select the device\n",
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:22:57.084159Z",
     "iopub.status.busy": "2024-05-17T19:22:57.083813Z",
     "iopub.status.idle": "2024-05-17T19:23:04.793963Z",
     "shell.execute_reply": "2024-05-17T19:23:04.793228Z",
     "shell.execute_reply.started": "2024-05-17T19:22:57.084137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "facebook/roberta-large is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/facebook/roberta-large/resolve/main/vocab.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:428\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    429\u001b[0m         path_or_repo_id,\n\u001b[1;32m    430\u001b[0m         filename,\n\u001b[1;32m    431\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    432\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    433\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    434\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    435\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    436\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    437\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    438\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    439\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    440\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1197\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1198\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1199\u001b[0m         timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1200\u001b[0m     )\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[0;32m-> 1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:291\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    283\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     )\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-664afda3-5822c7ed21e97baa50ee0862)\n\nRepository Not Found for url: https://huggingface.co/facebook/roberta-large/resolve/main/vocab.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the model and tokenizer for RoBERTa large\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/roberta-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m RobertaForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1813\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m download_url(file_path, proxies\u001b[38;5;241m=\u001b[39mproxies)\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1813\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   1814\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1815\u001b[0m             file_path,\n\u001b[1;32m   1816\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1817\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1818\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1819\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   1820\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1821\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1822\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   1823\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1824\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   1825\u001b[0m             _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1826\u001b[0m             _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1827\u001b[0m             _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   1828\u001b[0m         )\n\u001b[1;32m   1829\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unresolved_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:449\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: facebook/roberta-large is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Define the model and tokenizer for RoBERTa large\n",
    "model_name = \"facebook/roberta-large\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # Assuming 3 labels for the task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:04.796605Z",
     "iopub.status.busy": "2024-05-17T19:23:04.796158Z",
     "iopub.status.idle": "2024-05-17T19:23:14.611570Z",
     "shell.execute_reply": "2024-05-17T19:23:14.610711Z",
     "shell.execute_reply.started": "2024-05-17T19:23:04.796578Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# SNLI train dataset\n",
    "\n",
    "df_snli_train = pd.read_csv('SNLI/snli_1.0_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:14.612966Z",
     "iopub.status.busy": "2024-05-17T19:23:14.612682Z",
     "iopub.status.idle": "2024-05-17T19:23:14.640758Z",
     "shell.execute_reply": "2024-05-17T19:23:14.639890Z",
     "shell.execute_reply.started": "2024-05-17T19:23:14.612943Z"
    }
   },
   "outputs": [],
   "source": [
    "df_snli_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:14.642060Z",
     "iopub.status.busy": "2024-05-17T19:23:14.641727Z",
     "iopub.status.idle": "2024-05-17T19:23:15.313973Z",
     "shell.execute_reply": "2024-05-17T19:23:15.313014Z",
     "shell.execute_reply.started": "2024-05-17T19:23:14.642035Z"
    }
   },
   "outputs": [],
   "source": [
    "df_snli_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:15.315929Z",
     "iopub.status.busy": "2024-05-17T19:23:15.315327Z",
     "iopub.status.idle": "2024-05-17T19:23:15.482458Z",
     "shell.execute_reply": "2024-05-17T19:23:15.481393Z",
     "shell.execute_reply.started": "2024-05-17T19:23:15.315895Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNLI test dataset\n",
    "\n",
    "df_snli_test = pd.read_csv('SNLI/snli_1.0_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:15.484650Z",
     "iopub.status.busy": "2024-05-17T19:23:15.483934Z",
     "iopub.status.idle": "2024-05-17T19:23:15.504692Z",
     "shell.execute_reply": "2024-05-17T19:23:15.503792Z",
     "shell.execute_reply.started": "2024-05-17T19:23:15.484614Z"
    }
   },
   "outputs": [],
   "source": [
    "df_snli_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:15.506697Z",
     "iopub.status.busy": "2024-05-17T19:23:15.505926Z",
     "iopub.status.idle": "2024-05-17T19:23:15.532291Z",
     "shell.execute_reply": "2024-05-17T19:23:15.531250Z",
     "shell.execute_reply.started": "2024-05-17T19:23:15.506673Z"
    }
   },
   "outputs": [],
   "source": [
    "df_snli_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:15.535736Z",
     "iopub.status.busy": "2024-05-17T19:23:15.535092Z",
     "iopub.status.idle": "2024-05-17T19:23:21.373475Z",
     "shell.execute_reply": "2024-05-17T19:23:21.372272Z",
     "shell.execute_reply.started": "2024-05-17T19:23:15.535712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the data\n",
    "def load_and_preprocess_data(filepath):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Select only the required columns\n",
    "    df = df[['sentence1', 'sentence2', 'gold_label']]\n",
    "    \n",
    "    # Convert sentences to lowercase to standardize the text\n",
    "    df['sentence1'] = df['sentence1'].str.lower()\n",
    "    df['sentence2'] = df['sentence2'].str.lower()\n",
    "    \n",
    "    # Mapping labels to integers\n",
    "    label_mapping = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "    df['gold_label'] = df['gold_label'].map(label_mapping)\n",
    "    \n",
    "    # Drop rows with any missing values in these columns\n",
    "    df.dropna(subset=['sentence1', 'sentence2', 'gold_label'], inplace=True)\n",
    "    \n",
    "    # Ensure the gold_label column is of type integer\n",
    "    df['gold_label'] = df['gold_label'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# File paths\n",
    "train_path = 'SNLI/snli_1.0_train.csv'\n",
    "dev_path = 'SNLI/snli_1.0_dev.csv'\n",
    "\n",
    "# Load and preprocess the training and development datasets\n",
    "df_snli_train = load_and_preprocess_data(train_path)\n",
    "df_snli_dev = load_and_preprocess_data(dev_path)\n",
    "\n",
    "# Display the first few rows of the training data to verify\n",
    "print(df_snli_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:21.375265Z",
     "iopub.status.busy": "2024-05-17T19:23:21.374863Z",
     "iopub.status.idle": "2024-05-17T19:23:21.386407Z",
     "shell.execute_reply": "2024-05-17T19:23:21.385440Z",
     "shell.execute_reply.started": "2024-05-17T19:23:21.375229Z"
    }
   },
   "outputs": [],
   "source": [
    "df_snli_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:21.387808Z",
     "iopub.status.busy": "2024-05-17T19:23:21.387540Z",
     "iopub.status.idle": "2024-05-17T19:23:22.903979Z",
     "shell.execute_reply": "2024-05-17T19:23:22.903103Z",
     "shell.execute_reply.started": "2024-05-17T19:23:21.387786Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the distribution of labels\n",
    "label_counts = df_snli_train['gold_label'].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Labels in the Training Data')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:23:22.906005Z",
     "iopub.status.busy": "2024-05-17T19:23:22.905301Z",
     "iopub.status.idle": "2024-05-17T19:24:28.245752Z",
     "shell.execute_reply": "2024-05-17T19:24:28.244847Z",
     "shell.execute_reply.started": "2024-05-17T19:23:22.905970Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate the length of tokenized sentences\n",
    "def calculate_lengths(tokenizer, sentences):\n",
    "    lengths = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize and count the number of tokens\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        lengths.append(len(tokens))\n",
    "    return lengths\n",
    "\n",
    "# Concatenate the sentences from both fields for a more comprehensive overview\n",
    "all_sentences = df_snli_train['sentence1'].tolist() + df_snli_train['sentence2'].tolist()\n",
    "\n",
    "# Calculate lengths\n",
    "lengths = calculate_lengths(tokenizer, all_sentences)\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(lengths, bins=range(0, max(lengths) + 10, 5), alpha=0.75, color='blue')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Length of tokenized sentences')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:24:28.247350Z",
     "iopub.status.busy": "2024-05-17T19:24:28.246983Z",
     "iopub.status.idle": "2024-05-17T19:24:28.488281Z",
     "shell.execute_reply": "2024-05-17T19:24:28.487381Z",
     "shell.execute_reply.started": "2024-05-17T19:24:28.247315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: Plot distribution of labels to check balance\n",
    "import matplotlib.pyplot as plt\n",
    "label_counts = df_snli_dev['gold_label'].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Labels in the Validation Data')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:24:28.490093Z",
     "iopub.status.busy": "2024-05-17T19:24:28.489759Z",
     "iopub.status.idle": "2024-05-17T19:26:43.444187Z",
     "shell.execute_reply": "2024-05-17T19:26:43.443223Z",
     "shell.execute_reply.started": "2024-05-17T19:24:28.490067Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_and_prepare_inputs(df, tokenizer, max_length=45):\n",
    "    # Tokenize, pad and create attention masks\n",
    "    # 'max_length' is a parameter you can tune depending on the maximum length you observed in your dataset analysis\n",
    "    tokenized_inputs = tokenizer(\n",
    "        df['sentence1'].tolist(),\n",
    "        df['sentence2'].tolist(),\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Extracting input_ids and attention masks from the tokenized inputs\n",
    "    input_ids = tokenized_inputs['input_ids']\n",
    "    attention_masks = tokenized_inputs['attention_mask']\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize training data\n",
    "train_input_ids, train_attention_masks = tokenize_and_prepare_inputs(df_snli_train, tokenizer)\n",
    "\n",
    "# Tokenize validation data\n",
    "validation_input_ids, validation_attention_masks = tokenize_and_prepare_inputs(df_snli_dev, tokenizer)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(df_snli_train['gold_label'].values)\n",
    "validation_labels = torch.tensor(df_snli_dev['gold_label'].values)\n",
    "\n",
    "# Print some details to verify everything is as expected\n",
    "print(\"Training input IDs shape:\", train_input_ids.shape)\n",
    "print(\"Validation input IDs shape:\", validation_input_ids.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "print(\"Validation labels shape:\", validation_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:26:43.445518Z",
     "iopub.status.busy": "2024-05-17T19:26:43.445323Z",
     "iopub.status.idle": "2024-05-17T19:26:43.455930Z",
     "shell.execute_reply": "2024-05-17T19:26:43.455245Z",
     "shell.execute_reply.started": "2024-05-17T19:26:43.445496Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SNLIDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    \n",
    "train_dataset = SNLIDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "validation_dataset = SNLIDataset(validation_input_ids, validation_attention_masks, validation_labels)\n",
    "\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T19:26:43.457506Z",
     "iopub.status.busy": "2024-05-17T19:26:43.457244Z",
     "iopub.status.idle": "2024-05-17T19:27:24.855803Z",
     "shell.execute_reply": "2024-05-17T19:27:24.854643Z",
     "shell.execute_reply.started": "2024-05-17T19:26:43.457468Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW \n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "# CLR Scheduler setup (Ensure only one scheduler setup is active)\n",
    "scheduler = CyclicLR(optimizer, base_lr=5e-6, max_lr=1e-5, \n",
    "                     step_size_up=len(train_loader)//2, step_size_down=len(train_loader)//2,\n",
    "                     mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# Training and validation functions\n",
    "def train_model(model, train_loader, validation_loader, optimizer, scheduler, epochs=3, save_path='model_snapshots'):\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch')\n",
    "\n",
    "        for batch_index, batch in enumerate(progress_bar):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate after each batch\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            correct += (predicted == batch['labels']).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "            accuracy = 100. * correct / total\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "            progress_bar.set_postfix(loss=train_loss/(batch_index + 1), accuracy=f'{accuracy:.2f}%', lr=f'{lr:.6f}')\n",
    "\n",
    "            # Save snapshot at the end of each cycle\n",
    "            current_step = epoch * steps_per_epoch + batch_index\n",
    "            if (current_step + 1) % scheduler.total_size == 0:\n",
    "                torch.save(model.state_dict(), f'{save_path}/roberta_large_cycle_{current_step + 1}.pth')\n",
    "                print(f\"Snapshot saved at step {current_step + 1}\")\n",
    "\n",
    "        validate_model(model, validation_loader, device)\n",
    "\n",
    "def validate_model(model, validation_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_loader, desc='Validating', leave=False, unit='batch'):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            correct += (predicted == batch['labels']).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "\n",
    "    val_accuracy = 100. * correct / total\n",
    "    print(f'\\nValidation Loss: {val_loss / len(validation_loader):.4f} | Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Start training\n",
    "train_model(model, train_loader, validation_loader, optimizer, scheduler, epochs=5, save_path='/storage/data/st1070263')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1670,
     "sourceId": 2933,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2655798,
     "sourceId": 4548821,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2656775,
     "sourceId": 4550791,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4771616,
     "sourceId": 8083662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4771621,
     "sourceId": 8083668,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4771629,
     "sourceId": 8083678,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4772442,
     "sourceId": 8084913,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
